import torch
import torch.nn as nn
import snntorch as snn
from snntorch import surrogate
from timm.models.layers import trunc_normal_

beta = 0.5
spike_grad = surrogate.fast_sigmoid()


# new_input_shape: T * L(C) * D
# old_input_shape: T * C * H * W

class spiking_self_attention(nn.Module):
    # SSA
    def __init__(self, in_channels, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):
        super().__init__()
        assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."
        self.dim = dim
        self.num_heads = num_heads

        # self.q_conv = nn.Conv1d(dim, dim, kernel_size=1, stride=1,bias=False)
        self.q_linear = nn.Linear(dim, dim)
        self.q_bn = nn.BatchNorm1d(in_channels)
        self.q_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)

        self.k_linear = nn.Linear(dim, dim)
        self.k_bn = nn.BatchNorm1d(in_channels)
        self.k_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)

        self.v_linear = nn.Linear(dim, dim)
        self.v_bn = nn.BatchNorm1d(in_channels)
        self.v_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)

        self.attn_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)

        self.proj_linear = nn.Linear(dim, dim)
        self.proj_bn = nn.BatchNorm1d(in_channels)
        self.proj_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)

    def forward(self, x):  # new_input_shape : T * L * D
        # T,B,C,H,W = x.shape
        # x = x.flatten(3)
        # T, B, C, N = x.shape
        T, L, D = x.shape
        # x_for_qkv = x.flatten(0, 1)
        x_for_qkv = x

        q_conv_out = self.q_linear(x_for_qkv)
        q_conv_out = self.q_bn(q_conv_out).reshape(T, L, D).contiguous()
        q_conv_out = self.q_lif(q_conv_out)
        q = q_conv_out.transpose(-1, -2).reshape(T, D, self.num_heads, L//self.num_heads).permute(0, 2, 1, 3).contiguous()
        #  T, self.num_heads, D, L//self.num_heads

        k_conv_out = self.k_linear(x_for_qkv)
        k_conv_out = self.k_bn(k_conv_out).reshape(T, L, D).contiguous()
        k_conv_out = self.k_lif(k_conv_out)
        k = k_conv_out.transpose(-1, -2).reshape(T, D, self.num_heads, L//self.num_heads).permute(0, 2, 1, 3).contiguous()

        v_conv_out = self.v_linear(x_for_qkv)
        v_conv_out = self.v_bn(v_conv_out).reshape(T, L, D).contiguous()
        v_conv_out = self.v_lif(v_conv_out)
        v = v_conv_out.transpose(-1, -2).reshape(T, D, self.num_heads, L//self.num_heads).permute(0, 2, 1, 3).contiguous()

        attn = (q @ k.transpose(-2, -1))
        x = (attn @ v) * 0.125

        x = x.transpose(2, 3).reshape(T, L, D).contiguous()
        x = self.attn_lif(x)
        # x = x.flatten(0, 1)
        x = self.proj_lif(self.proj_bn(self.proj_linear(x)).reshape(T, L, D))

        return x


class MLP(nn.Module):
    def __init__(self, in_channels, in_features, hidden_features=None, out_features=None, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.fc1_conv = nn.Linear(in_features, hidden_features)
        self.fc1_bn = nn.BatchNorm1d(in_channels)
        self.fc1_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)

        self.fc2_conv = nn.Linear(hidden_features, out_features)
        self.fc2_bn = nn.BatchNorm1d(in_channels)
        self.fc2_lif = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)

        self.c_hidden = hidden_features
        self.c_output = out_features

    def forward(self, x):
        T, L, D = x.shape
        x = self.fc1_conv(x)
        x = self.fc1_bn(x).reshape(T, L, D).contiguous()
        x = self.fc1_lif(x)

        x = self.fc2_conv(x)
        x = self.fc2_bn(x).reshape(T, L, D).contiguous()
        x = self.fc2_lif(x)
        return x


class encoder_block(nn.Module):
    def __init__(self, in_channels, dim, num_heads=8, qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = spiking_self_attention(in_channels, dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                                           attn_drop=attn_drop, proj_drop=drop)

        # self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)

        self.mlp = MLP(in_channels=in_channels, in_features=dim, drop=drop)

    def forward(self, x):
        x = x + self.attn(x)
        x = x + self.mlp(x)
        return x


class spikformer(nn.Module):
    def __init__(self, in_channels, dim, num_heads, num_classes, depths):
        super(spikformer, self).__init__()
        self.num_classes = num_classes
        self.depths = depths

        block = nn.ModuleList([
            encoder_block(in_channels=in_channels, dim=dim, num_heads=num_heads)
            for i in range(depths)
        ])
        setattr(self, f"block", block)

        self.head = nn.Linear(dim, num_classes) if num_classes > 0 else nn.Identity()
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward_feature(self, x):
        block = getattr(self, f"block")
        for blk in block:
            x = blk(x)
        return x

    def forward(self, x):
        x = self.forward_feature(x)
        x = self.head(x)
        return x


if __name__ == '__main__':
    x = torch.randn(2, 8, 256)
    model = spikformer(in_channels=8, dim=256, num_heads=8, num_classes=10, depths=6)
    model.eval()
    y = model(x)
    print(y.shape)
