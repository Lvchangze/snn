import torch
import torch.nn as nn
from spikingjelly.activation_based import neuron

# input_shape: T * B * L * D
class spiking_self_attention(nn.Module):
    def __init__(self, length, dim, heads=8, qkv_bias=False, qk_scale=0.25, ):
        super().__init__()
        assert dim % heads == 0, f"dim {dim} should be divided by num_heads {heads}."

        self.dim = dim
        self.heads = heads
        self.qk_scale = qk_scale

        self.q_m = nn.Linear(dim, dim)
        self.q_bn = nn.BatchNorm1d(length)
        self.q_lif = neuron.LIFNode(step_mode='m')

        self.k_m = nn.Linear(dim, dim)
        self.k_bn = nn.BatchNorm1d(length)
        self.k_lif = neuron.LIFNode(step_mode='m')

        self.v_m = nn.Linear(dim, dim)
        self.v_bn = nn.BatchNorm1d(length)
        self.v_lif = neuron.LIFNode(step_mode='m')

        self.attn_lif = neuron.LIFNode(v_threshold=0.5,  step_mode='m')

        self.last_m = nn.Linear(dim, dim)
        self.last_bn = nn.BatchNorm1d(length)
        self.last_lif = neuron.LIFNode(step_mode='m')

    def forward(self, x):  #x_shape: T * B * L * D
        T, B, L, D = x.shape
        x_for_qkv = x.flatten(0, 1)

        q_m_out = self.q_m(x_for_qkv)
        q_m_out = self.q_bn(q_m_out).reshape(T, B, L, D).contiguous()
        q_m_out = self.q_lif(q_m_out)
        q = q_m_out.transpose(-1, -2).reshape(T, B, D, self.heads, L//self.heads).permute(0, 1, 3, 2, 4).contiguous()

        k_m_out = self.k_m(x_for_qkv)
        k_m_out = self.k_bn(k_m_out).reshape(T, B, L, D).contiguous()
        k_m_out = self.k_lif(k_m_out)
        k = k_m_out.transpose(-1, -2).reshape(T, B, D, self.heads, L // self.heads).permute(0, 1, 3, 2, 4).contiguous()

        v_m_out = self.v_m(x_for_qkv)
        v_m_out = self.v_bn(v_m_out).reshape(T, B, L, D).contiguous()
        v_m_out = self.v_lif(q_m_out)
        v = v_m_out.transpose(-1, -2).reshape(T, B, D, self.heads, L // self.heads).permute(0, 1, 3, 2, 4).contiguous()

        attn = (q @ k.transpose(-2, -1))
        x = (attn @ v) * self.qk_scale  # x_shape: T * B * heads * D * L//heads
        x = x.transpose(3, 4).reshape(T, B, -1, D).contiguous()
        x = self.attn_lif(x)

        x = self.last_m(x)
        x = x.flatten(0, 1)
        x = self.last_bn(x)
        x = self.last_lif(x.reshape(T, B, L, D).contiguous())

        return x  # x_shape: T * B * L *D


class mlp(nn.Module):
    def __init__(self, length, in_features, hidden_features=None, out_features=None, ):
        super().__init__()
        self.length = length
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.in_features = in_features
        self.hidden_features = hidden_features
        self.out_features = out_features

        self.fc1 = nn.Linear(in_features, hidden_features)
        self.bn1 = nn.BatchNorm1d(self.length)
        self.lif1 = neuron.LIFNode(step_mode='m')

        self.fc2 = nn.Linear(hidden_features, out_features)
        self.bn2 = nn.BatchNorm1d(self.length)
        self.lif2 = neuron.LIFNode(step_mode='m')

    def forward(self, x):
        T, B, L, D = x.shape
        x = x.flatten(0, 1)
        x = self.lif1(self.bn1(self.fc1(x)).reshape(T, B, L, D).contiguous())
        x = x.flatten(0, 1)
        x = self.lif2(self.bn2(self.fc2(x)).reshape(T, B, L, D).contiguous())
        return x


class block(nn.Module):
    def __init__(self, length, dim, heads=8, qkv_bias=False, qk_scale=0.25):
        super().__init__()
        self.attn = spiking_self_attention(length=length, dim=dim, heads=heads, qkv_bias=qkv_bias, qk_scale=qk_scale)
        self.mlp = mlp(length=length, in_features=dim)

    def forward(self, x):
        x = x + self.attn(x)
        x = x + self.mlp(x)
        return x


class spikformer(nn.Module):
    def __init__(self, depths, length, dim, num_classes, heads=8, qkv_bias=False, qk_scale=0.125):
        super().__init__()

        self.blokes = nn.ModuleList([block(
            length=length, dim=dim, heads=heads, qkv_bias=qkv_bias, qk_scale=qk_scale
        ) for _ in range(depths)])
        self.classifier = nn.Linear(dim, num_classes) if num_classes > 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm1d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

    def forward(self, x): # x_shape: T * B * L * D
        for blk in self.blokes:
            x = blk(x)
        x = self.classifier(x)
        return x

# test
if __name__ == "__main__":
    x = torch.randn(2, 4, 8, 256)
    net = spikformer(6, 8, 256, 10)
    net.eval()
    y = net(x)
    print(y.shape)

